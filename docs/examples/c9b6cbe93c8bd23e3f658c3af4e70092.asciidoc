// This file is autogenerated, DO NOT EDIT
// analysis/tokenizers/edgengram-tokenizer.asciidoc:264

[source, python]
----
resp = client.indices.create(
    index="my-index-000001",
    settings={
        "analysis": {
            "analyzer": {
                "autocomplete": {
                    "tokenizer": "autocomplete",
                    "filter": [
                        "lowercase"
                    ]
                },
                "autocomplete_search": {
                    "tokenizer": "lowercase"
                }
            },
            "tokenizer": {
                "autocomplete": {
                    "type": "edge_ngram",
                    "min_gram": 2,
                    "max_gram": 10,
                    "token_chars": [
                        "letter"
                    ]
                }
            }
        }
    },
    mappings={
        "properties": {
            "title": {
                "type": "text",
                "analyzer": "autocomplete",
                "search_analyzer": "autocomplete_search"
            }
        }
    },
)
print(resp)

resp1 = client.index(
    index="my-index-000001",
    id="1",
    document={
        "title": "Quick Foxes"
    },
)
print(resp1)

resp2 = client.indices.refresh(
    index="my-index-000001",
)
print(resp2)

resp3 = client.search(
    index="my-index-000001",
    query={
        "match": {
            "title": {
                "query": "Quick Fo",
                "operator": "and"
            }
        }
    },
)
print(resp3)
----
