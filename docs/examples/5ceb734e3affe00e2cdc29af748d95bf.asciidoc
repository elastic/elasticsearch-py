// This file is autogenerated, DO NOT EDIT
// inference/inference-apis.asciidoc:98

[source, python]
----
resp = client.inference.put(
    task_type="sparse_embedding",
    inference_id="small_chunk_size",
    inference_config={
        "service": "elasticsearch",
        "service_settings": {
            "num_allocations": 1,
            "num_threads": 1
        },
        "chunking_settings": {
            "strategy": "sentence",
            "max_chunk_size": 100,
            "sentence_overlap": 0
        }
    },
)
print(resp)
----
